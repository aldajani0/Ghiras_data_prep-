{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df504b1",
   "metadata": {},
   "source": [
    "## 03 - img Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfc481",
   "metadata": {},
   "source": [
    "Image processing techniques applied (applied directly to the original data):\n",
    "-  Noise Reduction (Bilateral Filter, Morphological Operations)\n",
    "- Color Space Conversion (RGB ↔ HSV ↔ LAB)\n",
    "- Image Enhancement \n",
    "- Edge Detection \n",
    "- Normalization \n",
    "- Aspect Ratio Preservation with Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e95d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "import cv2, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f0af1",
   "metadata": {},
   "source": [
    "Paths & settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33002ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Advanced processing for plant disease images (In-Place)\n",
      "Processing will be applied directly to the original data\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"/Users/amirah/Ghiras's datast/THE DATA\")\n",
    "WORK_DIR = Path(\"./plant_disease_project2\")\n",
    "LOGS_DIR = WORK_DIR / \"logs\"\n",
    "PROCESSING_DIR = WORK_DIR / \"processing_techniques\"\n",
    "\n",
    "for d in [LOGS_DIR, PROCESSING_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_SIZE = (224, 224) #Target size for each image after processing is 224×224.\n",
    "IMG_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Advanced processing for plant disease images (In-Place)\")\n",
    "print(\"Processing will be applied directly to the original data\") #The processing will overwrite the original images\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28b6aa",
   "metadata": {},
   "source": [
    "Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac99a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(p: Path) -> bool: #Check file extension\n",
    "    return p.suffix.lower() in IMG_EXTS\n",
    "\n",
    "def safe_imread(path: Path, flags=cv2.IMREAD_COLOR): # Safe image read \n",
    "    try:\n",
    "        img = cv2.imread(str(path), flags)  \n",
    "        if img is None or img.size == 0:\n",
    "            return None\n",
    "        return img\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def write_json(obj, path: Path): #Save JSON to disk\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd89600",
   "metadata": {},
   "source": [
    "Step 1 : Loading image processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30b7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Reduction \n",
    "def denoise_bilateral(img):   \n",
    "    try:\n",
    "        return cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Bilateral Filter failed: {e}\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5214ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast Enhancement - تحسين التباين\n",
    "#Implementation: split into 8×8 tiles; enhance each tile independently\n",
    "\n",
    "def enhance_contrast_clahe(img):              #CLAHE (Contrast Limited Adaptive Histogram Equalization)،Improves contrast locally (tile-wise)، Preserves natural colors\n",
    "    try:\n",
    "        # تحويل لـ LAB\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # تطبيق CLAHE على قناة Lightness\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l = clahe.apply(l)\n",
    "        \n",
    "        # دمج القنوات مرة أخرى\n",
    "        lab = cv2.merge([l, a, b])\n",
    "        result = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # التحقق من النتيجة\n",
    "        if result is None or result.size == 0:\n",
    "            return img\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] CLAHE failed: {e}\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect-Ratio-Preserving Resize \n",
    "def resize_with_padding(img, target_size=(224, 224)):  #Technique: Aspect Ratio Preservation with Padding\n",
    "\n",
    "    #Goal: retain the leaf’s original information\n",
    "    try:\n",
    "        h, w = img.shape[:2]\n",
    "        target_h, target_w = target_size\n",
    "        \n",
    "        # التحقق من الأبعاد الصحيحة\n",
    "        if h <= 0 or w <= 0:\n",
    "            return img\n",
    "        \n",
    "        # حساب أفضل scale (استخدام INTER_LINEAR بدل INTER_AREA للسرعة)\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        \n",
    "        # التحقق من الأبعاد الجديدة\n",
    "        if new_w <= 0 or new_h <= 0:\n",
    "            return img\n",
    "        \n",
    "        # تحجيم (INTER_LINEAR أسرع من INTER_AREA)\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # حساب الـ padding\n",
    "        pad_top = (target_h - new_h) // 2\n",
    "        pad_bottom = target_h - new_h - pad_top\n",
    "        pad_left = (target_w - new_w) // 2\n",
    "        pad_right = target_w - new_w - pad_left\n",
    "        \n",
    "        # إضافة padding بيضاء\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            resized,\n",
    "            pad_top, pad_bottom, pad_left, pad_right,\n",
    "            cv2.BORDER_CONSTANT,\n",
    "            value=[255, 255, 255]\n",
    "        )\n",
    "        \n",
    "        return padded\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Resize failed: {e}\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662b512",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb680738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "def normalize_to_uint8(img):           # Min-Max Scaling then cast to uint8, Scales values to [0, 255]\n",
    "                                      #Formula: (x - min) / (max - min) * 255\n",
    "    try:\n",
    "        img_min = np.min(img)\n",
    "        img_max = np.max(img)\n",
    "        \n",
    "        # التحقق من القيم\n",
    "        if img_max - img_min > 0:\n",
    "            normalized = (img.astype(np.float32) - img_min) / (img_max - img_min) * 255\n",
    "        else:\n",
    "            normalized = img.astype(np.float32)\n",
    "        \n",
    "        return np.uint8(np.clip(normalized, 0, 255))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Normalization failed: {e}\")\n",
    "        return np.uint8(np.clip(img, 0, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54c6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge Detection (illustrations only) \n",
    "def detect_edges_canny(img): #Includes noise suppression\n",
    "    try :\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        return edges\n",
    "    except Exception as e:\n",
    "        return np.zeros_like(gray, dtype=np.uint8)\n",
    "\n",
    "#Useful for outlining diseased regions\n",
    "#Convert to grayscale, then apply Canny with thresholds of 100, 200 and remap the edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e914fa",
   "metadata": {},
   "source": [
    "Step 2 : Initial dataset scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b875edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of classes: 96\n",
      "Total images: 83668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = sorted([d for d in DATA_PATH.iterdir() if d.is_dir()], key=lambda p: p.name)\n",
    "all_items = []\n",
    "\n",
    "for c in classes:\n",
    "    files = [p for p in c.iterdir() if is_image(p)]\n",
    "    for p in files:\n",
    "        all_items.append({\"disease_class\": c.name, \"path\": p})\n",
    "\n",
    "df_original = pd.DataFrame(all_items)           #Builds a DataFrame from the paths, and prints the total\n",
    "total_images = len(df_original)\n",
    "\n",
    "print(f\" Number of classes: {len(classes)}\") \n",
    "print(f\"Total images: {total_images}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77150e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image distribution (before processing):\n",
      "  • Apple_Apple_scab: 2520 images\n",
      "  • Apple_Black_rot: 2484 images\n",
      "  • Apple_Cedar_apple_rust: 2200 images\n",
      "  • Apple_healthy: 2500 images\n",
      "  • Blueberry_healthy: 1816 images\n",
      "  ... and 90 other classes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Class distribution before processing\n",
    "#The distribution of images is calculated according to the class before processing, and only the first 5 are displayed, while the rest are counted\n",
    "\n",
    "class_counts_before = df_original[\"disease_class\"].value_counts().sort_index() \n",
    "print(\"Image distribution (before processing):\")\n",
    "for disease, count in list(class_counts_before.items())[:5]:\n",
    "    print(f\"  • {disease}: {count} images\")\n",
    "print(f\"  ... and {len(class_counts_before) - 5} other classes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40c1d3",
   "metadata": {},
   "source": [
    "Applying processing to all images (in-place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41319647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in-place: 100%|██████████| 83668/83668 [09:29<00:00, 146.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Successfully processed images: 83668\n",
      " Failed images: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_count = 0\n",
    "failed_processing = []\n",
    "visualization_samples = []\n",
    "processing_log = []\n",
    "\n",
    "for idx, (_, row) in enumerate(tqdm(df_original.iterrows(), total=len(df_original), \n",
    "                                     desc=\"Processing images in-place\")):\n",
    "    path = Path(row[\"path\"])\n",
    "    disease_class = row[\"disease_class\"]\n",
    "    \n",
    "    # Read original image\n",
    "    original = safe_imread(path)\n",
    "    if original is None:\n",
    "        failed_processing.append(str(path))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Processing pipeline >> Apply in sequence: Bilateral → CLAHE → Resize+Padding → Normalize\n",
    "\n",
    "        # 1) Noise reduction (Bilateral Filter)\n",
    "        denoised = denoise_bilateral(original)\n",
    "        \n",
    "        # 2) Contrast enhancement (CLAHE)\n",
    "        enhanced = enhance_contrast_clahe(denoised)\n",
    "        \n",
    "        # 3) Aspect-ratio-preserving resize\n",
    "        resized = resize_with_padding(enhanced, TARGET_SIZE)\n",
    "        \n",
    "        # 4) Normalization\n",
    "        normalized = normalize_to_uint8(resized)\n",
    "        \n",
    "        # Save processed image over original (in-place) \n",
    "        cv2.imwrite(str(path), normalized)\n",
    "        \n",
    "        processed_count += 1 #Saves the result in place of the original image (In-Place) and increments the counter\n",
    "        \n",
    "        # Log processing\n",
    "        processing_log.append({\n",
    "            'image': path.name,\n",
    "            'disease_class': disease_class,\n",
    "            'original_shape': original.shape,\n",
    "            'processed_shape': normalized.shape,\n",
    "            'status': 'success'\n",
    "        }) #The processing log records the before/after volume and success status \n",
    "        \n",
    "        # Save illustration samples (roughly 9 examples)\n",
    "        if idx % (max(1, len(df_original) // 9)) == 0 and len(visualization_samples) < 9:\n",
    "            visualization_samples.append({\n",
    "                'disease': disease_class,\n",
    "                'original': original,\n",
    "                'denoised': denoised,\n",
    "                'enhanced': enhanced,\n",
    "                'resized': resized,\n",
    "                'normalized': normalized,\n",
    "                'edges': detect_edges_canny(normalized)\n",
    "            }) #Collects approximately 9 samples distributed over the entire dataset for illustrations: stores all stages of the image (original, after each step, and edges).\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_processing.append(str(path))\n",
    "        processing_log.append({\n",
    "            'image': path.name,\n",
    "            'disease_class': disease_class,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }) #If an error occurs, the failure is logged with an error message\n",
    "\n",
    "print(f\"\\n Successfully processed images: {processed_count}\")\n",
    "print(f\" Failed images: {len(failed_processing)}\\n\")\n",
    "\n",
    "if failed_processing:\n",
    "    write_json({\"failed_processing\": failed_processing}, LOGS_DIR / \"04_failed_processing.json\") #Prints the summary, and saves the list of failed images (if any) in JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc40413",
   "metadata": {},
   "source": [
    "Step 4 : Rescanning after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f9ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image distribution (after processing):\n",
      "  • Apple_Apple_scab: 2520 images\n",
      "  • Apple_Black_rot: 2484 images\n",
      "  • Apple_Cedar_apple_rust: 2200 images\n",
      "  • Apple_healthy: 2500 images\n",
      "  • Blueberry_healthy: 1816 images\n",
      "  ... and 90 other classes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_items_after = []\n",
    "for c in classes:\n",
    "    files = [p for p in c.iterdir() if is_image(p)]\n",
    "    for p in files:\n",
    "        all_items_after.append({\"disease_class\": c.name, \"path\": p})\n",
    "\n",
    "df_after = pd.DataFrame(all_items_after)\n",
    "class_counts_after = df_after[\"disease_class\"].value_counts().sort_index() #Scans the same folder structure again and calculates the distribution after processing (to ensure that the numbers have not changed)\n",
    "\n",
    "print(\"Image distribution (after processing):\")\n",
    "for disease, count in list(class_counts_after.items())[:5]:\n",
    "    print(f\"  • {disease}: {count} images\")\n",
    "print(f\"  ... and {len(class_counts_after) - 5} other classes\\n\") #Displays the first 5 items of the new distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347ed1a",
   "metadata": {},
   "source": [
    "Step 5: Generating illustration figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6356e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved pipeline figure: plant_disease_project2/processing_techniques/processing_pipeline_inplace.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if visualization_samples:\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    \n",
    "    for row_idx, sample in enumerate(visualization_samples):\n",
    "        # Original\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 1)\n",
    "        ax.imshow(cv2.cvtColor(sample['original'], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(f\"1. Original\\n{sample['disease'][:15]}\", fontsize=8, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Denoised (Bilateral Filter)\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 2)\n",
    "        ax.imshow(cv2.cvtColor(sample['denoised'], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(\"2. Denoised\\n(Bilateral Filter)\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Enhanced (CLAHE)\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 3)\n",
    "        ax.imshow(cv2.cvtColor(sample['enhanced'], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(\"3. Enhanced\\n(CLAHE)\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Resized (Aspect Ratio)\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 4)\n",
    "        ax.imshow(cv2.cvtColor(sample['resized'], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(f\"4. Resized\\n{TARGET_SIZE}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Normalized\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 5)\n",
    "        ax.imshow(sample['normalized'], cmap='viridis')\n",
    "        ax.set_title(\"5. Normalized\\n[0, 255]\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Edge Detection\n",
    "        ax = plt.subplot(9, 6, row_idx * 6 + 6)\n",
    "        ax.imshow(sample['edges'], cmap='gray')\n",
    "        ax.set_title(\"6. Edges\\n(Canny)\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Complete Image Processing Pipeline (Applied In-Place)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROCESSING_DIR / \"processing_pipeline_inplace.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\" Saved pipeline figure: {PROCESSING_DIR / 'processing_pipeline_inplace.png'}\\n\")\n",
    "    #Builds a large 9x6 board: for each sample, 6 images representing the processing stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26a56a",
   "metadata": {},
   "source": [
    "Step 6: Generating distribution statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178aec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved statistics: plant_disease_project2/processing_techniques/disease_distribution_after.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar chart - Top 20\n",
    "top_20 = class_counts_after.nlargest(20)\n",
    "axes[0].barh(range(len(top_20)), top_20.values, color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_20)))\n",
    "axes[0].set_yticklabels(top_20.index, fontsize=9)\n",
    "axes[0].set_xlabel(\"Number of Images\", fontsize=12)\n",
    "axes[0].set_title(\"Top 20 Diseases (After Processing)\", fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Pie chart - Top 15\n",
    "top_15 = class_counts_after.nlargest(15)\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(top_15)))\n",
    "wedges, texts, autotexts = axes[1].pie(top_15.values, labels=top_15.index, \n",
    "                                         autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontsize(8)\n",
    "for text in texts:\n",
    "    text.set_fontsize(8)\n",
    "axes[1].set_title(\"Top 15 Diseases Distribution\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSING_DIR / \"disease_distribution_after.png\", dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\" Saved statistics: {PROCESSING_DIR / 'disease_distribution_after.png'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62a325",
   "metadata": {},
   "source": [
    "Step 7: Building final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61638a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"project\": \"Plant Disease Detection - Graduation Project\",\n",
    "    \"phase\": \"Advanced Image Processing (In-Place)\",\n",
    "    \"processing_date\": datetime.now().isoformat(),\n",
    "    \"processing_location\": \"Applied directly to source data\",\n",
    "    \"source_path\": str(DATA_PATH),\n",
    "    \"statistics\": {\n",
    "        \"total_images_processed\": processed_count,\n",
    "        \"failed_processing\": len(failed_processing),\n",
    "        \"target_size\": TARGET_SIZE,\n",
    "        \"total_diseases\": len(class_counts_after)\n",
    "    },\n",
    "    \"processing_techniques\": {\n",
    "        \"1_denoising\": {\n",
    "            \"name\": \"Bilateral Filter\",\n",
    "            \"description\": \"Edge-preserving noise reduction\",\n",
    "            \"formula\": \"Gaussian filtering in spatial and color (range) domains\",\n",
    "            \"parameters\": {\"diameter\": 9, \"sigma_color\": 75, \"sigma_space\": 75}\n",
    "        },\n",
    "        \"2_contrast_enhancement\": {\n",
    "            \"name\": \"CLAHE\",\n",
    "            \"description\": \"Contrast Limited Adaptive Histogram Equalization\",\n",
    "            \"formula\": \"Split image into 8×8 tiles and enhance each tile independently\",\n",
    "            \"parameters\": {\"clip_limit\": 2.0, \"tile_grid_size\": 8}\n",
    "        },\n",
    "        \"3_resizing\": {\n",
    "            \"name\": \"Aspect Ratio Preserving Resize\",\n",
    "            \"description\": \"Resize with white padding to maintain proportions\",\n",
    "            \"formula\": \"Compute optimal scale + add white padding\",\n",
    "            \"target_size\": TARGET_SIZE\n",
    "        },\n",
    "        \"4_normalization\": {\n",
    "            \"name\": \"Min-Max Scaling\",\n",
    "            \"description\": \"Normalize pixel values to [0, 255]\",\n",
    "            \"formula\": \"(x - min) / (max - min) * 255\",\n",
    "            \"output_range\": \"[0, 255]\"\n",
    "        }\n",
    "    },\n",
    "    \"disease_distribution\": class_counts_after.to_dict(),\n",
    "    \"processing_log_samples\": processing_log[:10]\n",
    "}\n",
    "\n",
    "write_json(summary, LOGS_DIR / \"05_processing_summary_inplace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176a3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full processing log\n",
    "df_processing_log = pd.DataFrame(processing_log)\n",
    "df_processing_log.to_csv(LOGS_DIR / \"processing_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebaa407",
   "metadata": {},
   "source": [
    "Final summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90456151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " In-place processing completed successfully!\n",
      "================================================================================\n",
      "\n",
      " Summary:\n",
      "  • Processed images: 83668\n",
      "  • Failed images: 0\n",
      "  • Number of classes: 95\n",
      "  • Final target size: (224, 224)\n",
      "  • Final value range: [0, 255]\n",
      "\n",
      " Applied image processing techniques:\n",
      "Bilateral Filter — edge-preserving denoising\n",
      "CLAHE — local contrast enhancement\n",
      "Aspect-Ratio Preserving Resize — no geometric distortion\n",
      "Min-Max Normalization — scale to [0, 255]\n",
      "Canny Edge Detection — (illustrations only)\n",
      "\n",
      " Outputs:\n",
      "  • Processed data location: /Users/amirah/Ghiras's datast/THE DATA\n",
      "  • Illustration figures: plant_disease_project2/processing_techniques\n",
      "  • Logs: plant_disease_project2/logs\n",
      "\n",
      " Saved files:\n",
      "  • processing_pipeline_inplace.png — pipeline figure\n",
      "  • disease_distribution_after.png — distribution stats\n",
      "  • 05_processing_summary_inplace.json — comprehensive summary\n",
      "  • processing_log.csv — processing log\n",
      "\n",
      "✨ The original dataset is now processed and ready for training!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" In-place processing completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Summary:\")\n",
    "print(f\"  • Processed images: {processed_count}\")\n",
    "print(f\"  • Failed images: {len(failed_processing)}\")\n",
    "print(f\"  • Number of classes: {len(class_counts_after)}\")\n",
    "print(f\"  • Final target size: {TARGET_SIZE}\")\n",
    "print(f\"  • Final value range: [0, 255]\")\n",
    "\n",
    "print(f\"\\n Applied image processing techniques:\")\n",
    "print(f\"Bilateral Filter — edge-preserving denoising\")\n",
    "print(f\"CLAHE — local contrast enhancement\")\n",
    "print(f\"Aspect-Ratio Preserving Resize — no geometric distortion\")\n",
    "print(f\"Min-Max Normalization — scale to [0, 255]\")\n",
    "print(f\"Canny Edge Detection — (illustrations only)\")\n",
    "\n",
    "print(f\"\\n Outputs:\")\n",
    "print(f\"  • Processed data location: {DATA_PATH.resolve()}\")\n",
    "print(f\"  • Illustration figures: {PROCESSING_DIR}\")\n",
    "print(f\"  • Logs: {LOGS_DIR}\")\n",
    "\n",
    "print(f\"\\n Saved files:\")\n",
    "print(f\"  • processing_pipeline_inplace.png — pipeline figure\")\n",
    "print(f\"  • disease_distribution_after.png — distribution stats\")\n",
    "print(f\"  • 05_processing_summary_inplace.json — comprehensive summary\")\n",
    "print(f\"  • processing_log.csv — processing log\")\n",
    "\n",
    "print(f\"\\n✨ The original dataset is now processed and ready for training!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
